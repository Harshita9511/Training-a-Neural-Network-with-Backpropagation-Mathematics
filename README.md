# Training a Neural Network with Backpropagation - Mathematics
Backpropagation is a widely used algorithm in training feedforward neural networks for supervised learning. It computes the gradient of the loss function with respect to the different weights and bias by using the chain rule of differential calculus. These gradients are used to update the weights and bias. Since these gradients are learned in the backward direction, starting from the output node, this learning process is referred to as the backward propagation.
This post attempts to explain how backpropagation works with a concrete example - https://inblog.in/Part-2-Training-a-Neural-Network-with-Backpropagation-Mathematics-bdyEJx6bKU
